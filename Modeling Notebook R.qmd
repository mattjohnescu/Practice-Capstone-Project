---
title: "Practice Capstone Modeling Notebook"
author: "Matthew Johnescu"
format: html
editor: visual
---


```{r setup, include=TRUE}
knitr::opts_chunk$set(warning = FALSE)

library(pacman)
p_load(dplyr, caret, pROC, lightgbm)

```


```{r}
#load in training and test data
train_data <- read.csv("C:/Users/johne/Downloads/home-credit-default-risk/application_train.csv", stringsAsFactors = TRUE)
test_data <- read.csv("C:/Users/johne/Downloads/home-credit-default-risk/application_test.csv")
cleanedTrainData <- train_data
cleanedTrainData <- test_data
```


# Cleaning Data

```{r}
set.seed(123)
# Function to clean data
clean_data <- function(data) {
  # Get list of all factor columns
  factor_cols <- names(data)[sapply(data, is.factor)]
  # Replace the null values with a value of "missing"
  data[factor_cols][is.na(data[factor_cols])] <- "missing"
  data$isCashLoan <- ifelse(data$NAME_CONTRACT_TYPE %in% c("Cash loans"), 1, 0)
  # Convert married to new column that includes 1 if in married or civil marriage and 0 otherwise
  data$isMarried <- ifelse(data$NAME_FAMILY_STATUS %in% c("Married", "Civil marriage"), 1, 0)
  # Creating variable for more than secondary education
  data$morethanSecondaryEd <- ifelse(data$NAME_EDUCATION_TYPE %in% c("Higher education", "Incomplete higher", "Academic degree"), 1, 0)
  # Creating variable for secondary or lower education
  data$SecondaryorLowerEd <- ifelse(data$NAME_EDUCATION_TYPE %in% c("Secondary / secondary special", "Lower secondary"), 1, 0)
  #making factor column for isCashLoan
  # Create an anomalous group - where days employed exceeds 100 years
  data$DAY_EMPLOYED_ANOM <- ifelse(data$DAYS_EMPLOYED >= 36500, 1, 0)
  # Remove the anomalous groups from the data
  data$DAYS_EMPLOYED[data$DAYS_EMPLOYED > 36500] <- NA
  # Create column for years old instead of days old for readability
  data$YearsOld <- data$DAYS_BIRTH / -365
  # Divide the age data in bins for every 10 years
  data$age_group <- cut(data$YearsOld, breaks = seq(min(data$YearsOld, na.rm = TRUE), max(data$YearsOld, na.rm = TRUE), by = 10))
  return(data)
}

# Clean train and test data
cleanedTrainData <- clean_data(train_data)
cleanedTestData <- clean_data(test_data)

summary(cleanedTrainData)
summary(cleanedTestData$target)
```

# Split Data and Define Target Variable

```{r}
# Split training data into training and validation sets (New Step: Splitting Data for Evaluation)
trainIndex <- createDataPartition(cleanedTrainData$TARGET, p = 0.8, list = FALSE)
trainSet <- cleanedTrainData[trainIndex, ]
validationSet <- cleanedTrainData[-trainIndex, ]

# Define the target variable and feature columns
target_variable <- "TARGET"
features <- setdiff(names(cleanedTrainData), target_variable)
```


# Prepare Model 

```{r}
set.seed(123)
# Prepare data for LightGBM
train_matrix <- lgb.Dataset(data = as.matrix(trainSet[, features]), label = trainSet[, target_variable], free_raw_data = FALSE)
validation_matrix <- lgb.Dataset(data = as.matrix(validationSet[, features]), label = validationSet[, target_variable], free_raw_data = FALSE)

# Set parameters for LightGBM
params <- list(
  objective = "binary",
  boosting_type = "gbdt",
  num_leaves = 50,
  learning_rate = 0.05,
  feature_fraction = 0.9
)
```

Comment:
- 
- 
- fff

# Training Model with Cross-Validation

```{r}
# Cross-Validation (Step 4: Cross-Validation)
# cv_results <- lgb.cv(params = params, data = train_matrix, nrounds = 500, nfold = 10, stratified = TRUE, eval = "auc", verbose = -1)
```


# Early Stopping
```{r}
# lgb_model <- lgb.train(params = params, data = train_matrix, nrounds = 500, valids = list(validation = validation_matrix), early_stopping_rounds = 50, verbose = -1)

```

Commnet:
- Early Stopping was incorperated to see if the number of rounds could be reduced or increased. 
- Computing time takes longer depending on the number of rounds because of the large number of features.

# Plotting Feature Importance
```{r}
# Feature Importance Plot (Step 6: Feature Importance)
#importance <- lgb.importance(lgb_model)
# lgb.plot.importance(importance, top_n = 25)

```

# Adjusting for class imbalance
```{r}
# Address Class Imbalance (Step 5: Class Imbalance)
params$scale_pos_weight <- sum(trainSet$TARGET == 0) / sum(trainSet$TARGET == 1)
```

Comment:
- Adjusting for the imbalance in the class

# Re-train model with updated parameters
```{r}
# Re-train model with updated parameters
lgb_model <- lgb.train(params = params, data = train_matrix, nrounds = 1500, valids = list(validation = validation_matrix), early_stopping_rounds = 500, verbose = 1)

```




# Evaluation Metrics
```{r}
# Evaluation Metrics (New Step: Evaluation Metrics)
roc_auc <- roc(validationSet$TARGET, predict(lgb_model, as.matrix(validationSet[, features])))$auc
cat("ROC AUC: ", roc_auc, "\n")
```
#### First .7189512


# Confusion Matrix
```{r}
# Confusion Matrix (New Step: Confusion Matrix)
predicted_classes <- ifelse(predict(lgb_model, as.matrix(validationSet[, features])) > 0.5, 1, 0)
conf_matrix <- table(Predicted = predicted_classes, Actual = validationSet$TARGET)
print(conf_matrix)
```
Notes:
- First Confusion Matrix: 
      Actual
Predicted     0     1
        0 56564  4938

## Get Kaggle Submission Score

```{r}
# Load sample submission file
sample_submission <- read.csv("C:/Users/johne/Downloads/sample_submission.csv")

# Add predictions to the sample submission
sample_submission$TARGET <- predictions

# Save the updated submission file
write.csv(sample_submission, "C:/Users/johne/Downloads/submissionLIGHTGBM4.csv", row.names = FALSE)

# FIRST SUBMISSION SCORE: 0.73977
# SECOND SUBMISSION SCORE: 0.74085
```


