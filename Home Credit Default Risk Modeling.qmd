---
title: "Modeling Notebook Group 9"
author: "Matt Johnescu, Dan Powell, Melissa Messervy"
format: html
editor: visual
---

# Table of Contents:

-   [Introduction](#introdution)

-   [Loading Packages](#loading-packages)

-   [Loading Data](#loading-data)

-   [Cleaning Data](#cleaning-data)

-   [LightGBM Model](#lightgbm-model)

    -   [Prepping Model](#prepping-model)
    -   [Running Model with Cross-Validation](#runnig-model-with-cross-validation)
    -   [Early Stopping](#early-stopping)
    -   [Plotting Feature Importance](#plotting-feature-importance)
    -   [Adjusting for Class Imbalance](#adjusting-for-class-imbalance)
    -   [Re-Train Model with Updated Parameters](#re-train-model-with-updated-parameters)
    -   [Non-Kaggle Evaluation Score](#non-kaggle-evaluation-score)
    -   [LightGBM Conclusions](#lightgbm-conclusions)

-   [Support Vector Machine](#support-vector-machine)

    -   [Feature Selection and SVM Data Preparation](#feature-selection-and-svm-model-preparation)
    -   [Simple Linear Model](#simple-linear-model)
        -   [Simple 3-Fold Cross Validation with Linear SVM Model](simple3-fold-cross-validation-with-linear-svm-model)
        -   [SMOTE Linear SVM Model](smote-linear-svm-model)
    -   [Test Data](test-data)
    -   [SVM Conclusions](svm-conclusions)

-   [Linear Regression](#linear-regression)

    -   [Clean Train and Test Data for LR](#clean-train-and-test-data-for-LR)
    -   [Impute Missing Data in Both Train and Test Datasets](#impute-missing-data-in-both-train-and-test-datasets)
    -   [Run Linear Regression Models](#run-linear-regression-models)
    -   [Generate each Models Evaluation Metrics (MAE, RMSE, MAPE, RMSPE, RAE, R2) for test and training](#generate-each-models-evaluation-metrics)

# Introduction:

This notebook aims to develop a predictive model to determine loan repayment ability for individuals with limited or no traditional credit history, addressing Home Credit's challenge of serving underserved populations. By utilizing alternative data sources such as telco and transactional information, we aim to enhance the loan approval process, minimize defaults, and expand access to responsible credit.

The goal is to increase loan approval rates for creditworthy individuals, reduce default rates, and broaden financial inclusion while providing optimized loan terms. These improvements will help Home Credit expand its client base responsibly and increase profitability.

The notebook explores supervised machine learning techniques, focusing on classification models to predict loan default probability using both traditional and alternative data. The main performance metric is the Area Under the ROC Curve (AUC-ROC), along with metrics like increased loan approvals and reduced defaults.

We will compare the performance of Light Gradient Boosting Machine (LightGBM), Support Vector Machine (SVM), and Logistic Regression models to identify the best approach. Issues such as class imbalance, cross-validation, and feature importance are addressed to ensure a robust and accurate final model.

By combining these approaches, this notebook aims to improve loan approval accuracy and support Home Credit's mission to expand responsible access to credit.

# Loading Packages {#loading-packages}

```{r setup, include=TRUE, warning=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
options(repos = c(CRAN = "https://cloud.r-project.org"))

# Load essential data manipulation and matrix operation packages
library(dplyr)
library(matrixStats)

# Load machine learning and modeling packages
library(caret)
library(lightgbm)
library(performanceEstimation)
library(ranger)
library(rpart)
library(rminer)

# Load evaluation and performance metrics packages
library(pROC)
library(kernlab)

# Load other utility packages
library(knitr)
```

# Loading Data {#loading-data}

```{r}
#load in training and test data
train_data <- read.csv("C:/Users/johne/Downloads/home-credit-default-risk/application_train.csv", stringsAsFactors = TRUE)
test_data <- read.csv("C:/Users/johne/Downloads/home-credit-default-risk/application_test.csv")
cleanedTrainData <- train_data
```

# Cleaning Data {#cleaning-data}

```{r}
set.seed(123)
# Function to clean data
clean_data <- function(data) {
  # Get list of all factor columns
  factor_cols <- names(data)[sapply(data, is.factor)]
  # Replace the null values with a value of "missing"
  data[factor_cols][is.na(data[factor_cols])] <- "missing"
  data$isCashLoan <- ifelse(data$NAME_CONTRACT_TYPE %in% c("Cash loans"), 1, 0)
  # Convert married to new column that includes 1 if in married or civil marriage and 0 otherwise
  data$isMarried <- ifelse(data$NAME_FAMILY_STATUS %in% c("Married", "Civil marriage"), 1, 0)
  # Creating variable for more than secondary education
  data$morethanSecondaryEd <- ifelse(data$NAME_EDUCATION_TYPE %in% c("Higher education", "Incomplete higher", "Academic degree"), 1, 0)
  # Creating variable for secondary or lower education
  data$SecondaryorLowerEd <- ifelse(data$NAME_EDUCATION_TYPE %in% c("Secondary / secondary special", "Lower secondary"), 1, 0)
  #making factor column for isCashLoan
  # Create an anomalous group - where days employed exceeds 100 years
  data$DAY_EMPLOYED_ANOM <- ifelse(data$DAYS_EMPLOYED >= 36500, 1, 0)
  # Remove the anomalous groups from the data
  data$DAYS_EMPLOYED[data$DAYS_EMPLOYED > 36500] <- NA
  # Create column for years old instead of days old for readability
  data$YearsOld <- data$DAYS_BIRTH / -365
  # Divide the age data in bins for every 10 years
  data$age_group <- cut(data$YearsOld, breaks = seq(min(data$YearsOld, na.rm = TRUE), max(data$YearsOld, na.rm = TRUE), by = 10))
  return(data)
}

# Clean train and test data
cleanedTrainData <- clean_data(train_data)
cleanedTestData <- clean_data(test_data)
head(cleanedTestData)
head(cleanedTrainData)
```

# LightGBM Model {#lightgbm-model}

## Prepping Model {#prepping-model}

```{r}
# Set seed for reproducibility
set.seed(123)

# Split training data into training and validation sets (New Step: Splitting Data for Evaluation)
trainIndex <- createDataPartition(cleanedTrainData$TARGET, p = 0.8, list = FALSE)
trainSet <- cleanedTrainData[trainIndex, ]
validationSet <- cleanedTrainData[-trainIndex, ]

# Define the target variable and feature columns
target_variable <- "TARGET"
features <- setdiff(names(cleanedTrainData), target_variable)

# Prepare data for LightGBM
train_matrix <- lgb.Dataset(data = as.matrix(trainSet[, features]), label = trainSet[, target_variable], free_raw_data = FALSE)
validation_matrix <- lgb.Dataset(data = as.matrix(validationSet[, features]), label = validationSet[, target_variable], free_raw_data = FALSE)

# Set parameters for LightGBM
params <- list(
  objective = "binary",
  boosting_type = "gbdt",
  num_leaves = 31,
  learning_rate = 0.05,
  feature_fraction = 0.9
)
```

## Running Model with Cross Validation

```{r}
# Cross-Validation
cv_results <- lgb.cv(params = params, data = train_matrix, nrounds = 500, nfold = 5, stratified = TRUE, eval = "auc", verbose = -1)
print(cv_results)
```

## Early Stopping {#early-stopping}

```{r}
# Early Stopping
lgb_model <- lgb.train(params = params, data = train_matrix, nrounds = 500, valids = list(validation = validation_matrix), early_stopping_rounds = 100, verbose = -1)
```

## Plotting Feature Importance {#plotting-feature-importance}

```{r}
# Feature Importance Plot
importance <- lgb.importance(lgb_model)
lgb.plot.importance(importance, top_n = 20)
```

## Adjusting for the class Imbalance

```{r}
# Address Class Imbalance
params$scale_pos_weight <- sum(trainSet$TARGET == 0) / sum(trainSet$TARGET == 1)
```

-   This may help performance becuase of the large imbalance in classes found in EDA

## Re-train model with updated parameters {#re-train-model-with-updated-parameters}

```{r}
# Set seed for reproducibility
set.seed(123)
# Re-train model with updated parameters
lgb_model <- lgb.train(params = params, data = train_matrix, nrounds = cv_results$best_iter, valids = list(validation = validation_matrix), early_stopping_rounds = 50, verbose = -1)
```

-   Updating and running model based on conclusions from tryong different model levers
-   Implemented early stopping

## Non-Kaggle Evaluation Score {#non-kaggle-evaluation-score}

```{r}
# Evaluation Metrics (New Step: Evaluation Metrics)
roc_auc <- roc(validationSet$TARGET, predict(lgb_model, as.matrix(validationSet[, features])))$auc
cat("ROC AUC: ", roc_auc, "\n")

# Confusion Matrix (New Step: Confusion Matrix)
predicted_classes <- ifelse(predict(lgb_model, as.matrix(validationSet[, features])) > 0.5, 1, 0)
conf_matrix <- table(Predicted = predicted_classes, Actual = validationSet$TARGET)
print(conf_matrix)
```

-   This is slightly different than Kaggle but represents the accuracy for this particular seed

## Get Kaggle Submission Score

```{r}
# Ensure both training and test data have consistent feature sets
features_in_test <- intersect(features, names(cleanedTestData))
missing_features <- setdiff(features, features_in_test)
for (feature in missing_features) {
  cleanedTestData[[feature]] <- 0
}

test_matrix <- as.matrix(cleanedTestData[, features])
# Make predictions on the test set
predictions <- predict(lgb_model, test_matrix)

# Load sample submission file
sample_submission <- read.csv("C:/Users/johne/Downloads/sample_submission (1).csv")

# Add predictions to the sample submission
sample_submission$TARGET <- predictions

# Save the updated submission file
write.csv(sample_submission, "C:/Users/johne/Downloads/rf_submission_6.csv", row.names = FALSE)

```

#### Results:

-   FIRST SUBMISSION SCORE: 0.73977
-   SECOND SUBMISSION SCORE: 0.68161
-   THIRD SUBMISSION SCORE: 0.73631
-   FOURTH SUBMISSION SCORE: 0.74085

## Summary:

The LightGBM section focuses on building a high-performing predictive model to determine loan repayment ability. The data is prepared by cleaning and encoding,then testing on multiple model iterations. LightGBM was chosen for its efficiency in handling large data sets and its ability to capture complex interactions between features.

The key steps were setting good hyper parameters, using 5 cross-validation for model tuning, and adding early stopping to avoid overfitting and figure out the upper limit for number of rounds. After model training, we analyzed feature importance to understand the driving factors for predictions in the model. To further enhance performance, we made adjustments to the class imbalance using scale_pos_weight.

The model is evaluated using AUC-ROC and a confusion matrix, achieving a solid score in distinguishing between defaulters and non-defaulters. The trained model is used to generate predictions for the test set, and Kaggle submission results are reported to measure performance against benchmarks. The highest submission score achieved was 0.74085, indicating promising predictive accuracy, but not as impressive as results could be using other csv files in the data set.

# Support Vector Machine {#support-vector-machine}

The next model we will use is support vector machine. We chose this method as SVM supports predicting binary outcomes and can be effective in high dimensional spaces. Additionally, SVM models can be robust to overfitting since they focus on a subset of datapoints (support vectors) to define the decision boundary. This can reduce overfitting, which a high-dimensional model can be prone to. Finally, we chose SVM since it can handle imbalanced data. Our dataset is imbalanced, with only \~8% of our observations in our training dataset being a default.

Here, we will run a few models. First, we will run a model using the top 10 most postively and negatively correlated values with a simple linear SVM. We will then use evaluation metrics such as percision, recall and F1 score; and a confusion matrix to identify if and how we can improve the model.

**Loading Data**

```{r}
#load in training and test data
train_data <- read.csv("C:/Users/johne/Downloads/home-credit-default-risk/application_train.csv", stringsAsFactors = TRUE)
test_data <- read.csv("C:/Users/johne/Downloads/home-credit-default-risk/application_test.csv")
```

**Cleaning Data**

```{r}
set.seed(123)
# Function to clean data
clean_data <- function(data) {
  # Get list of all factor columns
  factor_cols <- names(data)[sapply(data, is.factor)]
  # Replace the null values with a value of "missing"
  data[factor_cols][is.na(data[factor_cols])] <- "missing"
  data$isCashLoan <- ifelse(data$NAME_CONTRACT_TYPE %in% c("Cash loans"), 1, 0)
  # Convert married to new column that includes 1 if in married or civil marriage and 0 otherwise
  data$isMarried <- ifelse(data$NAME_FAMILY_STATUS %in% c("Married", "Civil marriage"), 1, 0)
  # Creating variable for more than secondary education
  data$morethanSecondaryEd <- ifelse(data$NAME_EDUCATION_TYPE %in% c("Higher education", "Incomplete higher", "Academic degree"), 1, 0)
  # Creating variable for secondary or lower education
  data$SecondaryorLowerEd <- ifelse(data$NAME_EDUCATION_TYPE %in% c("Secondary / secondary special", "Lower secondary"), 1, 0)
  #making factor column for isCashLoan
  # Create an anomalous group - where days employed exceeds 100 years
  data$DAY_EMPLOYED_ANOM <- ifelse(data$DAYS_EMPLOYED >= 36500, 1, 0)
  # Remove the anomalous groups from the data
  data$DAYS_EMPLOYED[data$DAYS_EMPLOYED > 36500] <- NA
  # Create column for years old instead of days old for readability
  data$YearsOld <- data$DAYS_BIRTH / -365
  # Divide the age data in bins for every 10 years
  data$age_group <- cut(data$YearsOld, breaks = seq(min(data$YearsOld, na.rm = TRUE), max(data$YearsOld, na.rm = TRUE), by = 10))
  return(data)
}

# Clean train and test data
cleanedTrainData <- clean_data(train_data)
cleanedTestData <- clean_data(test_data)

# Clean train and test data
cleanedTrainData <- clean_data(train_data)
cleanedTestData <- clean_data(test_data)
```

## Feature Selection and SVM Data Preparation

```{r}
nullandcorrDF <- sapply(cleanedTrainData, function(column) {
  if (is.numeric(column)) {
    # Calculate percent of nulls
    percent_nulls <- mean(is.na(column)) * 100

    # Calculate correlation with target, excluding NAs
    correlation <- cor(cleanedTrainData$TARGET, column, use = "complete.obs")

    return(c(Percent_Nulls = percent_nulls, Correlation_with_Target = correlation))
  } else {
    return(c(Percent_Nulls = NA, Correlation_with_Target = NA))
  }
})

# Transpose to make it easier to read, then convert to data frame
nullandcorrDF <- as.data.frame(t(nullandcorrDF))

# Order by absolute value of correlation and then by least nulls
ordered_results <- nullandcorrDF %>%
  filter(!is.na(Correlation_with_Target)) %>%
  filter(Percent_Nulls <= 0.1, (Correlation_with_Target >= .01 | Correlation_with_Target <= -.01)) %>%
  arrange(Percent_Nulls, desc(abs(Correlation_with_Target)))
rownames<- rownames(ordered_results)
print(rownames)

```

## Simple Linear Model {#simple-linear-model}

```{r}
#creating top 10 and bottom 10 correlated variable dataset
top20train <- cleanedTrainData[c(rownames, 'age_group')]
top20test <- cleanedTestData[c(rownames[-1], 'age_group')]
summary(top20train)
```

To prepare the data, we first omit any nulls. As our variables were chosen not to have any nulls or, at least, a small percentage of nulls, we should not have too many rows removed on this step.

We then one-hot encode our categorical variables, which in this case is just the age_group. We then rename these columns to be more intuitive.

Next, we encode the target variable to be a "Y" or "N" value in order to make performance metrics such as confusion matrices easier to read in the future. After which, we factor the target variable to make it compatible with our SVM model.

Finally, we scale our factors to ensure that the magnitude in our variables does not influence their significance to the model. After all of this is complete, we then divide our data into training and test sets.

```{r}
SVMPrepare <- function(data, training_levels = NULL) {
  # Replace NAs with "missing" for all columns
  data[is.na(data)] <- "missing"
  
  # Align factor levels for all columns, if training levels are provided
  if (!is.null(training_levels)) {
    for (col_name in names(training_levels)) {
      if (col_name %in% colnames(data)) {
        data[[col_name]] <- factor(
          data[[col_name]],
          levels = training_levels[[col_name]]
        )
        
        # Replace NA introduced by invalid levels with "missing"
        data[[col_name]][is.na(data[[col_name]])] <- "missing"
      }
    }
  }

  # One-hot encode the `age_group` column
  if ("age_group" %in% colnames(data)) {
    dummies_model <- dummyVars(~ age_group, data = data)
    data_one_hot <- predict(dummies_model, newdata = data)
    data <- cbind(data, data_one_hot)
    data <- subset(data, select = -age_group)
  }
  
  return(data)
}




  #apply function to training and test for similar datasets
  SVMTrainData <- SVMPrepare(top20train)
  SVMTestData <- SVMPrepare(top20test)
  #rename columns for easier calls

  #transforming target 0 and 1 into factorable str
  SVMTrainData$TARGET[ SVMTrainData$TARGET == 1] <- "Y"
  SVMTrainData$TARGET[ SVMTrainData$TARGET == 0] <- "N"
  SVMTrainData$TARGET <- as.factor(SVMTrainData$TARGET)
  #scaling factors to have a mean of 0 with a SD of 1 as SVM is sensitive to the scale of input features
  SVMTrainScale <- scale(SVMTrainData[,-1])
  SVMTrainScale <- as.data.frame(SVMTrainScale)
  SVMTrainScale$TARGET <- SVMTrainData$TARGET
  SVMTrainData <- SVMTrainScale

  # Create train/test split on train data to evaluate model performance
  train_indices <- createDataPartition(SVMTrainData$TARGET, p = 0.8, list = FALSE)

  # Create train and test datasets
  SVMtrain_data <- SVMTrainData[train_indices, ]
  SVMtest_data <- SVMTrainData[-train_indices, ]

```

### Simple 3-Fold Cross Validation with Linear SVM Model

To begin our analysis, we first run a 5-fold-cross-validated SVM model on our target variable and predictors. We use a probability threshold of .5 to make our predictions, as this is usually the default value. At this stage, we are simply seeing how a basic model performs and then making further adjustments after we gather our findings.

```{r}

# Load required packages
# install.packages('e1071')
library(caret)
library(e1071)
library(doParallel)

# Set up parallel processing - leave two cores free
# cl <- makeCluster(detectCores() - 2)
#registerDoParallel(cl)

# Set up cross-validation with SMOTE and fewer folds for faster execution
train_control <- trainControl(
  method = "cv",
  number = 2,  # Using only 2 folds for faster cross-validation
  classProbs = TRUE,
  summaryFunction = twoClassSummary,
  sampling = "smote"  # Apply SMOTE sampling
)

# Use `svmLinear2` (a faster alternative for linear SVMs)
#set.seed(123)
#smote_model <- train(
#  TARGET ~ .,
#  data = smoteSVM_Train,
#  method = "svmLinear2",  # Faster linear SVM variant
#  trControl = train_control,
#  probability = TRUE,
#  tuneLength = 1  # Minimal tuning for speed
#)

# Predict using the trained model on the test data
# predictions <- predict(smote_model, newdata = SVMtest_data, type = "prob")$Y

# Set a threshold of 0.35 for binary classification
# binary_predictions <- ifelse(predictions > 0.35, "Y", "N")

# Output confusion matrix and evaluation metrics
# confusion <- confusionMatrix(factor(binary_predictions), factor(SVMtest_data$TARGET), positive = "Y")
# print(confusion)

# Calculate precision, recall, and F1 score
# precision <- confusion$byClass['Pos Pred Value']
# recall <- confusion$byClass['Sensitivity']
# f1_score <- 2 * (precision * recall) / (precision + recall)

# cat("\nPrecision: ", precision)
# cat("\nRecall: ", recall)
# cat("\nF1 Score: ", round(f1_score, 4))

# Stop the parallel cluster
# stopCluster(cl)
# registerDoSEQ()


```

### SMOTE Linear SVM Model

While our SVM Model seemed to perfrom well for accuracy, this is primarily due to class imbalance, which can be observed through our non-existant F1 score. Currently, the model just predicts that all observations will be Non-Defaults. However, this is not a good model to use as it does not achieve the goal of identifying potential defaults at all. To rectify this, we will use smote to balance the classes and then try rerunning the linear model.

```{r}
set.seed(123)
# Load performance estimation for smote dataset creation
install.packages("performanceEstimation", repos = "https://cloud.r-project.org")
library(performanceEstimation)
print('Before Smote:\n')
print(table(SVMtrain_data$TARGET))

#run smote function, using oversampling by 40% (chosen to limit size of new dataset for computational efficency)

smoteSVM_Train <- smote(TARGET ~ ., data = SVMtrain_data, perc.over = .05, perc.under = .05)

# see target variable distribution after smote
print('After Smote:\n')
print(table(smoteSVM_Train$TARGET))
```

The smote model showed improvement when running with perc.over = 10, perc.under = 1,  in detecting loan defaults and now has values for our performance metrics. Notably, the recall of 0.41 indicates that our model correctly identifies 41% of the actual positive cases. However, due to a low Percision metric (higher rate of false positives) we still end up with a low F1 score. This is due to many False Positives, which in our scenario, could mean that a customer gets denied a loan when they may deserve one. As one of the company's main selling points is accessibility to loans - this model may not suit our needs. Perhaps the low F1 score could be caused by non-linear relationships between the variables - which the SVMLinear model likely struggles to fit. In order to test this, we will rerun our smote dataset through a SVMRadial Dataset to try to capture the relationships between the variables more effectively. We also weight defaulted loans 10x heavier than non-defaults, as this is our primary interest when investigating the dataset.

```{r}

set.seed(123)
Svm_smote_Radial <- train(TARGET ~ .,
                   data = smoteSVM_Train,  #still using smote training dataset
                   method = "svmRadial",  #use radial for non-linear relationships
                   trControl = train_control, #still using 3 fold cross validation
                   tuneLength = 1,
                   preProcess = c("center", "scale"),
                   weights = ifelse(smoteSVM_Train$TARGET == "N", .05, .05))

```

```{r}
set.seed(123)
predictions <- predict(Svm_smote_Radial, newdata = SVMtest_data, type = "prob")$Y
# Create a list of thresholds to test
thresholds <- seq(0, 1, by = 0.01)
#Optimal F1 Score Threshold Determination
calculate_f1 <- function(threshold) {
  predicted_classes <- ifelse(predictions >= threshold, "Y", "N")
  confusion <- confusionMatrix(factor(predicted_classes), factor(SVMtest_data$TARGET), positive = "Y")
  # get percision from our confusion matrix
  precision <- confusion$byClass['Pos Pred Value']
  # recall
  recall <- confusion$byClass['Sensitivity']
  # calculating f1 score from percision and recall
  f1_score <- 2 * (precision * recall) / (precision + recall)
  return(f1_score)
}

# Calculate F1-scores for all thresholds
f1_scores <- sapply(thresholds, calculate_f1)

# Get max f1_score
optimal_threshold <- thresholds[which.max(f1_scores)]
print(optimal_threshold)

#use optimal f1_score for our predictions
binary_predictions <- ifelse(predictions > optimal_threshold, "Y", "N")

## Assuming you have the true labels and predicted probabilities:
true_labels <- SVMtest_data$TARGET
predicted_probs <- predictions

# Create the ROC curve object
roc_obj <- roc(true_labels, predictions)

# Plot the precision-recall curve
plot(roc_obj, print.auc = FALSE)

#run and output confusion matrix
confusion <- confusionMatrix(factor(binary_predictions), factor(SVMtest_data$TARGET), positive = "Y")
print(confusion)

# get percision from our confusion matrix
precision <- confusion$byClass['Pos Pred Value']
cat('\nPercision: ', precision)

# recall
recall <- confusion$byClass['Sensitivity']
cat('\nRecall: ', recall)

# calculating f1 score from percision and recall
f1_score <- 2 * (precision * recall) / (precision + recall)
cat("\nF1 Score: ", round(f1_score,4))
```

After running the model, we see a slight improvement in F1 Score and an increased recall and percision from our linear model. This improvement is a result of our careful model tuning, including the use of a non-linear SVM, a 10:1 weighting of "Y" to "N" observations, and the optimization of the classification threshold. Our accuracy also improved without classifying too many observations into the no-default category. Finally, we print the ROC curve to see the relationship between specificity (True Negative Rate) and sensitivity (True Positive Rate). Ideally, the curve should hug the top left of the chart, indicating a both high true positive and negatives rates at different thresholds. While our ROC curve does not hug the top left, it does rise above the perfectly linear reference line for a random walk, meaning the model found some patterns in the data, even if they were just slightly above random.

## Test Data

```{r}
predictions_final_SVM <- predict(Svm_smote_Radial, newdata = SVMtest_data, type = "prob")$Y
binary_predictions_final_SVM <- ifelse(predictions_final_SVM > optimal_threshold, "Y", "N")

# Load sample submission file
sample_submission <- read.csv("C:/Users/johne/Downloads/sample_submission (1).csv")

# Add predictions to the sample submission
sample_submission$TARGET <- binary_predictions_final_SVM

# Save the updated submission file
write.csv(sample_submission, "C:/Users/johne/Downloads/sample_Submission_SVM.csv", row.names = FALSE)
```

## SVM Conclusions

Unfortunately, it would seem that perhaps an SVM model is not best suited for modeling the data. While we were able to extract some patterns from the data with our final smote-using-weighted-Radial model, we were only able to achieve an F1 Score of 0.19, meaning that the model is struggling with distinguishing default observations from non-default observations.

# Linear Regression {#linear-regression}

## Clean Train and Test Data for LR {#clean-train-and-test-data-for-lr}

```{r}
# Function to clean data - specific to linear regression
set.seed(123)
clean_data3 <- function(data) {
  # Get list of all factor columns
  factor_cols <- names(data)[sapply(data, is.factor)]
  # Replace the null values with a value of "missing"
  data[factor_cols][is.na(data[factor_cols])] <- "missing"
  data$isCashLoan <- ifelse(data$NAME_CONTRACT_TYPE %in% c("Cash loans"), 1, 0)
  # Convert married to new column that includes 1 if in married or civil marriage and 0 otherwise
  data$isMarried <- ifelse(data$NAME_FAMILY_STATUS %in% c("Married", "Civil marriage"), 1, 0)
  # Creating variable for more than secondary education
  data$morethanSecondaryEd <- ifelse(data$NAME_EDUCATION_TYPE %in% c("Higher education", "Incomplete higher", "Academic degree"), 1, 0)
  # Creating variable for secondary or lower education
  data$SecondaryorLowerEd <- ifelse(data$NAME_EDUCATION_TYPE %in% c("Secondary / secondary special", "Lower secondary"), 1, 0)
  #making factor column for isCashLoan
  # Create an anomalous group - where days employed exceeds 100 years
  data$DAY_EMPLOYED_ANOM <- ifelse(data$DAYS_EMPLOYED >= 36500, 1, 0)
  # Remove the anomalous groups from the data
  data$DAYS_EMPLOYED[data$DAYS_EMPLOYED > 36500] <- NA
  # Create column for years old instead of days old for readability
  data$YearsOld <- data$DAYS_BIRTH / -365
 # Set fixed age bins (for example: from 20 to 80 in 10-year intervals)
  age_bins <- seq(10, 100, by = 10)
  data$age_group <- cut(data$YearsOld, breaks = age_bins, right = FALSE, include.lowest = TRUE, labels = FALSE)

  return(data)
}

# Clean train and test data

cleanedTrainData <- clean_data3(train_data)
cleanedTestData <- clean_data3(test_data)

allTrain <- clean_data(train_data)

# Split training data into training and validation sets
trainIndex <- createDataPartition(cleanedTrainData$TARGET, p = 0.8, list = FALSE)
trainSet <- cleanedTrainData[trainIndex, ]
testSet <- cleanedTrainData[-trainIndex, ]

cleanedTrainData <- trainSet
cleanedTestData <- testSet

trainTarget <- trainSet$TARGET
testTarget <- testSet$TARGET
```

## Impute missing data in both train and test datasets {#impute-missing-data-in-both-train-and-test-datasets}

```{r}
# Impute train data
missing_counts <- colSums(is.na(cleanedTrainData))
missing_columns <- names(missing_counts[missing_counts > 0])

for (col in missing_columns) {
  if (is.numeric(cleanedTrainData[[col]])) {
    cleanedTrainData[[col]][is.na(cleanedTrainData[[col]])] <- mean(cleanedTrainData[[col]], na.rm = TRUE)
  } else {
    print(paste("Column", col, "is not numeric; skipping imputation."))
  }
}
for (col in missing_columns) {
  if (is.factor(cleanedTrainData[[col]]) || is.character(cleanedTrainData[[col]])) {
    mode_value <- as.character(names(sort(table(cleanedTrainData[[col]]), decreasing = TRUE)[1]))
    cleanedTrainData[[col]][is.na(cleanedTrainData[[col]])] <- mode_value
  }
}

for (col in missing_columns) {
  cleanedTrainData[[col]][is.na(cleanedTrainData[[col]])] <- mean(cleanedTrainData[[col]], na.rm = TRUE)
}
```

```{r}
# Impute test data
missing_counts <- colSums(is.na(cleanedTestData))
missing_columns <- names(missing_counts[missing_counts > 0])

for (col in missing_columns) {
  if (is.numeric(cleanedTestData[[col]])) {
    cleanedTestData[[col]][is.na(cleanedTestData[[col]])] <- mean(cleanedTestData[[col]], na.rm = TRUE)
  } else {
    print(paste("Column", col, "is not numeric; skipping imputation."))
  }
}
for (col in missing_columns) {
  if (is.factor(cleanedTestData[[col]]) || is.character(cleanedTestData[[col]])) {
    mode_value <- as.character(names(sort(table(cleanedTestData[[col]]), decreasing = TRUE)[1]))
    cleanedTestData[[col]][is.na(cleanedTestData[[col]])] <- mode_value
  }
}

for (col in missing_columns) {
  cleanedTestData[[col]][is.na(cleanedTestData[[col]])] <- mean(cleanedTestData[[col]], na.rm = TRUE)
}
```

## Run Linear Regression Models {#run-linear-regression-models}

```{r}
# Run a basic logistic regression
base_model = lm(TARGET ~ ., data = cleanedTrainData)

# Run an rpart model
rpart_model = rpart(TARGET ~ ., data = cleanedTrainData)

# Run a glm
glm_model = glm(TARGET ~ ., data = cleanedTrainData, family = binomial)

```

*Three* linear regression models were created to determine which test is the best predictor for the target. A base linear model was run along with an rpart model and a glm model. The results are below.

## Generate each model's evaluation metrics {#generate-each-models-evaluation-metrics}

**Metrics: (MAE, RMSE, MAPE, RMSPE, RAE, RRSE, R2) for test and training**

```{r}
# lm model
# Generate predictions
predictions_base_train <- predict(base_model, cleanedTrainData)
predictions_base_test <- predict(base_model, cleanedTestData)
# Metrics on train data
mmetric(trainTarget,predictions_base_train,c("MAE","RMSE","MAPE","RMSPE","RAE", "RRSE", "COR", "R2"))
# Metrics on test data
mmetric(testTarget,predictions_base_test,c("MAE","RMSE","MAPE","RMSPE","RAE", "RRSE", "COR", "R2"))

```

```{r}
# rpart model
# Generate predictions using the model tree
predictions_rpart_train <- predict(rpart_model, cleanedTrainData)
predictions_rpart_test <- predict(rpart_model, cleanedTestData)
# Metrics on train data
mmetric(trainTarget,predictions_rpart_train,c("MAE","RMSE","MAPE","RMSPE","RAE", "RRSE","COR", "R2"))
# Metrics on test data
mmetric(testTarget,predictions_rpart_test,c("MAE","RMSE","MAPE","RMSPE","RAE", "RRSE","COR", "R2"))

```

```{r}
# GLM model
# Generate predictions using the model tree
predictions_glm_train <- predict(glm_model, cleanedTrainData)
predictions_glm_test <- predict(glm_model, cleanedTestData)
# Metrics on train data
mmetric(trainTarget,predictions_glm_train,c("MAE","RMSE","MAPE","RMSPE","RAE","RRSE","COR","R2"))
# Metrics on test data
mmetric(testTarget,predictions_glm_test,c("MAE","RMSE","MAPE","RMSPE","RAE","RRSE","COR","R2"))
```

The lm and rpart models produced very similar metrics. A low RMSE is a good indicator that these models are doing a good job of predicting the target variable, and in both cases the RMSE is 0.26. However, the model's relative absolute error is quite high, indicating that the models error levels are much higher than the baseline. The GLM had a very high RMSE and RAE, and had the worst metrics of all models tested. The R-squared value for the lm model was 0.06 and for the rpart model was 0.02. These are extremely low values, meaning that only 2% or 6% of the variance is explained by these models, respectively. This suggests poor model performance in terms of prediction accuracy and fit. This leads to the conclusion that a linear model would not be the best fit for this data, but this will be checked after a cross-validation is run.

```{r}
# Load the sample submission file
 sample_submission <- read.csv("C:/Users/johne/Downloads/sample_submission (1).csv")

# Linear Model Predictions (Base Model)
# predictions_base_test <- predict(base_model, cleanedTestData)
# binary_predictions_base <- ifelse(predictions_base_test > 0.5, 1, 0)
# sample_submission$TARGET <- binary_predictions_base
# write.csv(sample_submission, "C:/Users/johne/Downloads/sample_Submission_LR_base.csv", row.names = FALSE)

# RPart Model Predictions
# predictions_rpart_test <- predict(rpart_model, cleanedTestData)
# binary_predictions_rpart <- ifelse(predictions_rpart_test > 0.5, 1, 0)
# sample_submission$TARGET <- binary_predictions_rpart
# write.csv(sample_submission, "C:/Users/johne/Downloads/sample_Submission_LR_rpart.csv", row.names = FALSE)

# GLM Model Predictions
# predictions_glm_test <- predict(glm_model, cleanedTestData, type = "response")
# binary_predictions_glm <- ifelse(predictions_glm_test > 0.5, 1, 0)
# sample_submission$TARGET <- binary_predictions_glm
# write.csv(sample_submission, "C:/Users/johne/Downloads/sample_Submission_LR_glm.csv", row.names = FALSE)
```

## Run a Cross-validation

A cross validation function is created below to validate the models run above.

```{r}
# cross validation function
cv_function <- function(df, target, nFolds, seedVal, prediction_method, metrics_list)
{
  # Create folds
  set.seed(seedVal)
  folds = createFolds(df[,target],nFolds)

  # Perform cross validation
  cv_results <- lapply(folds, function(x)
  {
    test_target <- df[x,target]
    test_input  <- df[x,-target]

    train_target <- df[-x,target]
    train_input <- df[-x,-target]

    prediction_model <- prediction_method(train_target~.,train_input)
    pred <- predict(prediction_model,test_input)
    return(mmetric(test_target,pred,metrics_list))
  })

  # Generate means and sds and show cv results, means and sds using kable
  cv_results_m <- as.matrix(as.data.frame(cv_results))
  cv_mean<- as.matrix(rowMeans(cv_results_m))
  cv_sd <- as.matrix(rowSds(cv_results_m))
  colnames(cv_mean) <- "Mean"
  colnames(cv_sd) <- "Sd"
  cv_all <- cbind(cv_results_m, cv_mean, cv_sd)
  kable(t(cv_all),digits=2)
}
```

```{r}
# df <- allTrain
# target <- 8
# nFolds <- 5
# seedVal <- 123
# metrics_list <- c("MAE","RMSE","MAPE","RMSPE","RAE","RRSE","R2")

library(matrixStats)
library(knitr)

# lm
# assign("prediction_method", lm)
# cv_function(df, target, nFolds, seedVal, prediction_method, metrics_list)

# rpart
# assign("prediction_method", rpart)
# cv_function(df, target, nFolds, seedVal, prediction_method, metrics_list)

# GLM
# assign("prediction_method", glm)
# cv_function(df, target, nFolds, seedVal, prediction_method, metrics_list)
```

Unfortunately, the cross validation function will not run with the lm and GLM functions. We are able to see from the rpart cross validation that the results of the cross validation are consistent with those of the model predictions and evaluations. The RMSE is quite large, meaning the error rate is high. The relative absolute error is slightly lower in the cross evaluation than the initial predictions, however it is still quite high. The R-squared value dropped to 0.01, meaning only 1% of variance is explained in the mode. This leads to the conclusion that a linear regression model is not a good fit for predicting the target for the home credit default risk.

**Group members and their contribution**

-   Data Cleaning: Dan, Matt, Melissa
-   LightGBM: Matt
-   SVM: Dan
-   Linear Regression: Melissa
-   Notebook Combination: Matt
-   Prof Use of Nottebook: Team
